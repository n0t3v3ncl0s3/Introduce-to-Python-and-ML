{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3c7ae8e",
      "metadata": {
        "id": "a3c7ae8e"
      },
      "source": [
        "# Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "15b685de",
      "metadata": {
        "id": "15b685de"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import random_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8687dc",
      "metadata": {
        "id": "5e8687dc"
      },
      "source": [
        "# Setup Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f224bfa9",
      "metadata": {
        "id": "f224bfa9"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a7211510",
      "metadata": {
        "id": "a7211510"
      },
      "outputs": [],
      "source": [
        "class_mapping = {\n",
        "    0: 1,\n",
        "    1: 0,\n",
        "}\n",
        "train_dir = r\"../datas/data\"\n",
        "test_dir = r\"../datas/test\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc7ecc0a",
      "metadata": {
        "id": "dc7ecc0a"
      },
      "source": [
        "## Build dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6155cde6",
      "metadata": {
        "id": "6155cde6"
      },
      "outputs": [],
      "source": [
        "class MushroomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, is_test=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.samples = []\n",
        "        self.classes = []\n",
        "        self.class_to_idx = {}\n",
        "\n",
        "        if not os.path.exists(root_dir):\n",
        "            print(f\"Warning: Directory {root_dir} does not exist.\")\n",
        "            return\n",
        "        try:\n",
        "            items = os.listdir(root_dir)\n",
        "            if self.is_test or any(item.lower().endswith(('.png', '.jpg', '.jpeg')) for item in items):\n",
        "                self._setup_test_dataset(root_dir)\n",
        "            else:\n",
        "                self._setup_train_dataset(root_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up dataset: {e}\")\n",
        "            self.classes = [\"unknown\"]\n",
        "            self.class_to_idx = {\"unknown\": 0}\n",
        "\n",
        "    def _setup_test_dataset(self, root_dir):\n",
        "        self.classes = [\"unknown\"]\n",
        "        self.class_to_idx = {\"unknown\": 0}\n",
        "\n",
        "        for img_name in sorted(os.listdir(root_dir)):\n",
        "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(root_dir, img_name)\n",
        "                if os.path.isfile(img_path):\n",
        "                    self.samples.append((img_path, -1))\n",
        "\n",
        "    def _setup_train_dataset(self, root_dir):\n",
        "        self.classes = sorted([d for d in os.listdir(root_dir)\n",
        "                              if os.path.isdir(os.path.join(root_dir, d))])\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "\n",
        "        for class_name in self.classes:\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            if os.path.isdir(class_dir):\n",
        "                for img_name in os.listdir(class_dir):\n",
        "                    img_path = os.path.join(class_dir, img_name)\n",
        "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        self.samples.append((img_path, self.class_to_idx[class_name]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            placeholder = torch.zeros((3, 32, 32))\n",
        "            return placeholder, label\n",
        "\n",
        "class MultiAugmentDataset(Dataset):\n",
        "    def __init__(self, dataset, num_copies=2, transforms_list=None):\n",
        "        self.dataset = dataset\n",
        "        self.num_copies = num_copies\n",
        "\n",
        "        if transforms_list is None:\n",
        "            self.transforms_list = [dataset.transform] * num_copies\n",
        "        else:\n",
        "            assert len(transforms_list) == num_copies, \"Number of transforms must match num_copies\"\n",
        "            self.transforms_list = transforms_list\n",
        "\n",
        "        self.original_transform = dataset.transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) * self.num_copies\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = idx % len(self.dataset)\n",
        "        copy_idx = idx // len(self.dataset)\n",
        "\n",
        "        self.dataset.transform = self.transforms_list[copy_idx]\n",
        "        image, label = self.dataset[real_idx]\n",
        "        self.dataset.transform = self.original_transform\n",
        "\n",
        "        return image, label\n",
        "\n",
        "def setup_data_loaders(train_dir, test_dir, batch_size=32, val_split=0.1,\n",
        "                       transforms_list=None, eval_transform=None, use_multi_augment=True):\n",
        "    print(\"Setting up training and test datasets with mixed augmentation strategies...\")\n",
        "\n",
        "    if not os.path.exists(train_dir):\n",
        "        print(f\"Warning: Training directory {train_dir} does not exist!\")\n",
        "        os.makedirs(train_dir, exist_ok=True)\n",
        "\n",
        "    train_dataset = MushroomDataset(train_dir, transform=None)\n",
        "\n",
        "    if use_multi_augment:\n",
        "        print(\"Using multi-augmentation strategy (6 copies with different transforms)\")\n",
        "        train_dataset = MultiAugmentDataset(\n",
        "            train_dataset,\n",
        "            num_copies=5,\n",
        "            transforms_list=transforms_list\n",
        "        )\n",
        "    else:\n",
        "        train_dataset.transform = transforms_list[0]\n",
        "\n",
        "    train_size = int((1 - val_split) * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "\n",
        "    train_dataset, valid_dataset = random_split(\n",
        "        train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    test_dataset = MushroomDataset(test_dir, transform=eval_transform, is_test=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(valid_dataset)}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "    return train_loader, valid_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "908e50d6",
      "metadata": {
        "id": "908e50d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up training and test datasets with mixed augmentation strategies...\n",
            "Using multi-augmentation strategy (6 copies with different transforms)\n",
            "Training samples: 2700\n",
            "Validation samples: 300\n",
            "Test samples: 100\n"
          ]
        }
      ],
      "source": [
        "train_loader, valid_loader, test_loader = setup_data_loaders(\n",
        "    train_dir, test_dir,\n",
        "    batch_size=32,\n",
        "    val_split=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d22efe2",
      "metadata": {
        "id": "7d22efe2"
      },
      "source": [
        "# Build Model SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "44d8734a",
      "metadata": {
        "id": "44d8734a"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "    X, y = [], []\n",
        "    for img, label in dataset:                  \n",
        "        vec = np.array(img).flatten()                           \n",
        "\n",
        "        X.append(vec)\n",
        "        y.append(label)\n",
        "    return np.stack(X), np.array(y)\n",
        "\n",
        "X_train, y_train = preprocess_dataset(train_loader.dataset)\n",
        "X_val,   y_val   = preprocess_dataset(valid_loader.dataset)\n",
        "X_test,  y_test  = preprocess_dataset(test_loader.dataset)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val   = scaler.transform(X_val)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "my_model = SVC(kernel='linear')\n",
        "my_model.fit(X_train, y_train)\n",
        "\n",
        "my_model.scaler_ = scaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cf36bc81",
      "metadata": {
        "id": "cf36bc81",
        "outputId": "ab4c2e75-76c8-4bee-bf42-66746dcc4259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to svm_model.pth\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Sau khi huấn luyện xong\n",
        "model_path = \"svm_model.pth\"\n",
        "joblib.dump(my_model, model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2546b3bb",
      "metadata": {
        "id": "2546b3bb"
      },
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "af19760a",
      "metadata": {
        "id": "af19760a"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    expected_labels = []\n",
        "    expected_labels.extend([1] * 50)\n",
        "    expected_labels.extend([0] * 50)\n",
        "\n",
        "    actual_labels = df['label'].tolist()\n",
        "\n",
        "    n = min(len(actual_labels), len(expected_labels))\n",
        "\n",
        "    correct = sum(1 for i in range(n) if actual_labels[i] == expected_labels[i])\n",
        "\n",
        "    accuracy = (correct / n) * 100\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "443d258d",
      "metadata": {
        "id": "443d258d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def predict_and_create_submission(model, test_loader, class_mapping, feature_extractor=None, size=(32,32), filename='submission.csv'):\n",
        "    # Extract filenames from dataset\n",
        "    dataset = test_loader.dataset\n",
        "    try:\n",
        "        all_filenames = [os.path.basename(path) for path, _ in dataset.samples]\n",
        "    except Exception:\n",
        "        all_filenames = [f\"test_image_{i}.jpg\" for i in range(len(dataset))]\n",
        "\n",
        "    # Prepare feature matrix\n",
        "    features = []\n",
        "    for idx, item in enumerate(dataset.samples):\n",
        "        path, _ = item\n",
        "        img = Image.open(path).convert('RGB').resize(size)\n",
        "        arr = np.array(img)\n",
        "        feat = feature_extractor(arr) if feature_extractor else arr.flatten()\n",
        "        features.append(feat)\n",
        "\n",
        "    X_test = np.vstack(features)\n",
        "\n",
        "    # Scale if scaler_ attribute exists\n",
        "    if hasattr(model, 'scaler_'):\n",
        "        X_test = model.scaler_.transform(X_test)\n",
        "\n",
        "    # Predict labels\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # Map to actual label strings\n",
        "    labels = [class_mapping.get(p, p) for p in preds]\n",
        "\n",
        "    # Build submission DataFrame\n",
        "    ids = [os.path.splitext(fn)[0] for fn in all_filenames]\n",
        "    submission_df = pd.DataFrame({'id': ids, 'label': labels})\n",
        "\n",
        "    # Show sample and save\n",
        "    print(\"\\nSubmission sample (before saving):\")\n",
        "    print(submission_df.head(10))\n",
        "    submission_df.to_csv(filename, index=False)\n",
        "    print(f\"Submission file created: {filename}\")\n",
        "\n",
        "    # Verify saved file\n",
        "    try:\n",
        "        saved = pd.read_csv(filename)\n",
        "        if not submission_df.equals(saved):\n",
        "            print(\"WARNING: Saved file differs from generated DataFrame!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error verifying saved file: {e}\")\n",
        "\n",
        "    return submission_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1b8c096c",
      "metadata": {
        "id": "1b8c096c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Submission sample (before saving):\n",
            "    id  label\n",
            "0  101      1\n",
            "1  102      0\n",
            "2  103      0\n",
            "3  104      1\n",
            "4  105      1\n",
            "5  106      0\n",
            "6  107      0\n",
            "7  108      1\n",
            "8  109      0\n",
            "9  110      1\n",
            "Submission file created: submission.csv\n",
            "WARNING: Saved file differs from generated DataFrame!\n"
          ]
        }
      ],
      "source": [
        "submission_df = predict_and_create_submission(my_model, test_loader, class_mapping, filename='submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "864d8659",
      "metadata": {
        "id": "864d8659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final evaluation on TEST set: 73.0\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nFinal evaluation on TEST set:\", calculate_accuracy('submission.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9a6cffb7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.7.1+cpu\n",
            "CUDA available: False\n",
            "CUDA toolkit version: None\n",
            "cuDNN version: None\n",
            "TorchVision version: 0.22.1+cpu\n",
            "NumPy version: 2.3.1\n",
            "scikit‑learn version: 1.6.1\n",
            "joblib version: 1.5.1\n",
            "Pillow (PIL) version: 11.3.0\n"
          ]
        }
      ],
      "source": [
        "import torchvision, sklearn, PIL\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA toolkit version:\", torch.version.cuda)\n",
        "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
        "\n",
        "print(\"TorchVision version:\", torchvision.__version__)\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "print(\"scikit‑learn version:\", sklearn.__version__)\n",
        "print(\"joblib version:\", joblib.__version__)\n",
        "print(\"Pillow (PIL) version:\", PIL.__version__)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
